{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75366af8-307f-474a-85a2-05ed24e9eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.skip):\n",
    "            next_state, reward, done, trunc, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return next_state, total_reward, done, trunc, info\n",
    "    \n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = SkipFrame(env, skip=4) # Num of frames to apply one action to\n",
    "    env = ResizeObservation(env, shape=84) # Resize frame from 240x256 to 84x84\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = FrameStack(env, num_stack=4, lz4_compress=True) # May need to change lz4_compress to False if issues arise\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922b282c-c925-48eb-a157-90bbbc2deb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class AgentNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, freeze=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.network = nn.Sequential(\n",
    "            self.conv,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "        \n",
    "        self.device = 'cpu'\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def _freeze(self):        \n",
    "        for p in self.network.parameters():\n",
    "            p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1aad31-57ce-4a02-99f7-8f421e9b535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 input_dims, \n",
    "                 num_actions, \n",
    "                 lr=0.0005, \n",
    "                 gamma=0.9, \n",
    "                 epsilon=1.0, \n",
    "                 eps_decay=0.99999975, \n",
    "                 eps_min=0.1, \n",
    "                 replay_buffer_capacity=100_000, \n",
    "                 batch_size=32, \n",
    "                 sync_network_rate=10000):\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.batch_size = batch_size\n",
    "        self.sync_network_rate = sync_network_rate\n",
    "\n",
    "        # Networks\n",
    "        self.online_network = AgentNN(input_dims, num_actions)\n",
    "        self.target_network = AgentNN(input_dims, num_actions, freeze=True)\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.SmoothL1Loss() \n",
    "\n",
    "        # Replay buffer\n",
    "        storage = LazyMemmapStorage(replay_buffer_capacity)\n",
    "        self.replay_buffer = TensorDictReplayBuffer(storage=storage)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        # Passing in a list of numpy arrays is slower than creating a tensor from a numpy array\n",
    "        # Hence the `np.array(observation)` instead of `observation`\n",
    "        # observation is a LIST of numpy arrays because of the LazyFrame wrapper\n",
    "        # Unqueeze adds a dimension to the tensor, which represents the batch dimension\n",
    "        observation = torch.tensor(np.array(observation), dtype=torch.float32) \\\n",
    "                        .unsqueeze(0) \\\n",
    "                        .to(self.online_network.device)\n",
    "        # Grabbing the index of the action that's associated with the highest Q-value\n",
    "        return self.online_network(observation).argmax().item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
    "\n",
    "    def store_in_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(TensorDict({\n",
    "                                            \"state\": torch.tensor(np.array(state), dtype=torch.float32), \n",
    "                                            \"action\": torch.tensor(action),\n",
    "                                            \"reward\": torch.tensor(reward), \n",
    "                                            \"next_state\": torch.tensor(np.array(next_state), dtype=torch.float32), \n",
    "                                            \"done\": torch.tensor(done)\n",
    "                                          }, batch_size=[]))\n",
    "        \n",
    "    def sync_networks(self):\n",
    "        if self.learn_step_counter % self.sync_network_rate == 0 and self.learn_step_counter > 0:\n",
    "            self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.online_network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.online_network.load_state_dict(torch.load(f=path, weights_only = True))\n",
    "        self.target_network.load_state_dict(torch.load(f=path, weights_only = True))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.sync_networks()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        samples = self.replay_buffer.sample(self.batch_size).to(self.online_network.device)\n",
    "\n",
    "        keys = (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
    "\n",
    "        states, actions, rewards, next_states, dones = [samples[key] for key in keys]\n",
    "\n",
    "        predicted_q_values = self.online_network(states) # Shape is (batch_size, n_actions)\n",
    "        predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "\n",
    "        # Max returns two tensors, the first one is the maximum value, the second one is the index of the maximum value\n",
    "        target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "        # The rewards of any future states don't matter if the current state is a terminal state\n",
    "        # If done is true, then 1 - done is 0, so the part after the plus sign (representing the future rewards) is 0\n",
    "        target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "\n",
    "        loss = self.loss(predicted_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "        self.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f915dfd-610b-4fb9-889c-a130f3e42bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6f437e-dea3-480e-9009-cd4284b8c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_current_date_time_string():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\")\n",
    "\n",
    "\n",
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "\n",
    "    def start(self):\n",
    "        self.t = time.time()\n",
    "\n",
    "    def print(self, msg=''):\n",
    "        print(f\"Time taken: {msg}\", time.time() - self.t)\n",
    "\n",
    "    def get(self):\n",
    "        return time.time() - self.t\n",
    "    \n",
    "    def store(self):\n",
    "        self.times.append(time.time() - self.t)\n",
    "\n",
    "    def average(self):\n",
    "        return sum(self.times) / len(self.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ad5524-4ced-472a-ad35-2d8ca8fabd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", get_current_date_time_string())\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0'\n",
    "SHOULD_TRAIN = True\n",
    "DISPLAY = True\n",
    "CKPT_SAVE_INTERVAL = 500\n",
    "NUM_OF_EPISODES = 20_000\n",
    "\n",
    "env = gym_super_mario_bros.make(ENV_NAME, render_mode = None, apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n)\n",
    "\n",
    "if not SHOULD_TRAIN:\n",
    "    folder_name = \"\"\n",
    "    ckpt_name = \"\"\n",
    "    agent.load_model(os.path.join(\"models\", folder_name, ckpt_name))\n",
    "    agent.epsilon = 0.2\n",
    "    agent.eps_min = 0.0\n",
    "    agent.eps_decay = 0.0\n",
    "\n",
    "folder_name = \"2024-09-08-11_21_58\"\n",
    "ckpt_name = \"model_2077_iter.pt\"\n",
    "agent.load_model(os.path.join(\"models\", folder_name, ckpt_name))\n",
    "agent.epsilon = 0.7973863212825801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d32b7-d701-45b8-a769-69e6203cc412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aakar\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3564\n",
      "Total reward: 607.0 Epsilon: 0.7973697756860043 Size of replay buffer: 114 Learn step counter: 83\n",
      "Episode: 3565\n",
      "Total reward: 607.0 Epsilon: 0.7973482469900052 Size of replay buffer: 222 Learn step counter: 191\n",
      "Episode: 3566\n",
      "Total reward: 606.0 Epsilon: 0.7973313035176613 Size of replay buffer: 307 Learn step counter: 276\n",
      "Episode: 3567\n",
      "Total reward: 247.0 Epsilon: 0.7973233302434948 Size of replay buffer: 347 Learn step counter: 316\n",
      "Episode: 3568\n",
      "Total reward: 237.0 Epsilon: 0.7973147590626922 Size of replay buffer: 390 Learn step counter: 359\n",
      "Episode: 3569\n",
      "Total reward: 246.0 Epsilon: 0.7973063873006263 Size of replay buffer: 432 Learn step counter: 401\n",
      "Episode: 3570\n",
      "Total reward: 775.0 Epsilon: 0.7972386191294134 Size of replay buffer: 772 Learn step counter: 741\n",
      "Episode: 3571\n",
      "Total reward: 243.0 Epsilon: 0.7972314440132314 Size of replay buffer: 808 Learn step counter: 777\n",
      "Episode: 3572\n",
      "Total reward: 1012.0 Epsilon: 0.7971690630856164 Size of replay buffer: 1121 Learn step counter: 1090\n",
      "Episode: 3573\n",
      "Total reward: 443.0 Epsilon: 0.7967695821710227 Size of replay buffer: 3126 Learn step counter: 3095\n",
      "Episode: 3574\n",
      "Total reward: 990.0 Epsilon: 0.7966777597565555 Size of replay buffer: 3587 Learn step counter: 3556\n",
      "Episode: 3575\n",
      "Total reward: 195.0 Epsilon: 0.7966668055112989 Size of replay buffer: 3642 Learn step counter: 3611\n",
      "Episode: 3576\n",
      "Total reward: 1358.0 Epsilon: 0.796569219792659 Size of replay buffer: 4132 Learn step counter: 4101\n",
      "Episode: 3577\n",
      "Total reward: 219.0 Epsilon: 0.7965594618782628 Size of replay buffer: 4181 Learn step counter: 4150\n",
      "Episode: 3578\n",
      "Total reward: 595.0 Epsilon: 0.7965232192427382 Size of replay buffer: 4363 Learn step counter: 4332\n",
      "Episode: 3579\n",
      "Total reward: 585.0 Epsilon: 0.7965019125289351 Size of replay buffer: 4470 Learn step counter: 4439\n",
      "Episode: 3580\n",
      "Total reward: 247.0 Epsilon: 0.7964949431668193 Size of replay buffer: 4505 Learn step counter: 4474\n",
      "Episode: 3581\n",
      "Total reward: 979.0 Epsilon: 0.7962908674604114 Size of replay buffer: 5530 Learn step counter: 5499\n",
      "Episode: 3582\n",
      "Total reward: 604.0 Epsilon: 0.7962598127182657 Size of replay buffer: 5686 Learn step counter: 5655\n",
      "Episode: 3583\n",
      "Total reward: 1440.0 Epsilon: 0.79612227069894 Size of replay buffer: 6377 Learn step counter: 6346\n",
      "Episode: 3584\n",
      "Total reward: 1006.0 Epsilon: 0.7960520159997937 Size of replay buffer: 6730 Learn step counter: 6699\n",
      "Episode: 3585\n",
      "Total reward: 229.0 Epsilon: 0.7960434584855476 Size of replay buffer: 6773 Learn step counter: 6742\n",
      "Episode: 3586\n",
      "Total reward: 1623.0 Epsilon: 0.7959395815793318 Size of replay buffer: 7295 Learn step counter: 7264\n",
      "Episode: 3587\n",
      "Total reward: 619.0 Epsilon: 0.79591709660094 Size of replay buffer: 7408 Learn step counter: 7377\n",
      "Episode: 3588\n",
      "Total reward: 228.0 Epsilon: 0.7959097344009254 Size of replay buffer: 7445 Learn step counter: 7414\n",
      "Episode: 3589\n",
      "Total reward: 224.0 Epsilon: 0.7959003825153182 Size of replay buffer: 7492 Learn step counter: 7461\n",
      "Episode: 3590\n",
      "Total reward: 595.0 Epsilon: 0.7958711337100509 Size of replay buffer: 7639 Learn step counter: 7608\n",
      "Episode: 3591\n",
      "Total reward: 1687.0 Epsilon: 0.7955956109981499 Size of replay buffer: 9024 Learn step counter: 8993\n",
      "Episode: 3592\n",
      "Total reward: 635.0 Epsilon: 0.7955643844793353 Size of replay buffer: 9181 Learn step counter: 9150\n",
      "Episode: 3593\n",
      "Total reward: 1161.0 Epsilon: 0.7953500087393202 Size of replay buffer: 10259 Learn step counter: 10228\n",
      "Episode: 3594\n",
      "Total reward: 945.0 Epsilon: 0.7952239557371248 Size of replay buffer: 10893 Learn step counter: 10862\n",
      "Episode: 3595\n",
      "Total reward: 1546.0 Epsilon: 0.7950518083647896 Size of replay buffer: 11759 Learn step counter: 11728\n",
      "Episode: 3596\n",
      "Total reward: 629.0 Epsilon: 0.7950154355720394 Size of replay buffer: 11942 Learn step counter: 11911\n",
      "Episode: 3597\n",
      "Total reward: 225.0 Epsilon: 0.7950062929459564 Size of replay buffer: 11988 Learn step counter: 11957\n",
      "Episode: 3598\n",
      "Total reward: 250.0 Epsilon: 0.7949993366704563 Size of replay buffer: 12023 Learn step counter: 11992\n",
      "Episode: 3599\n",
      "Total reward: 240.0 Epsilon: 0.7949923804558234 Size of replay buffer: 12058 Learn step counter: 12027\n",
      "Episode: 3600\n",
      "Total reward: 582.0 Epsilon: 0.7949651524296683 Size of replay buffer: 12195 Learn step counter: 12164\n",
      "Episode: 3601\n",
      "Total reward: 1256.0 Epsilon: 0.7948620123795896 Size of replay buffer: 12714 Learn step counter: 12683\n",
      "Episode: 3602\n",
      "Total reward: 761.0 Epsilon: 0.794780145798271 Size of replay buffer: 13126 Learn step counter: 13095\n",
      "Episode: 3603\n",
      "Total reward: 651.0 Epsilon: 0.7947640516612584 Size of replay buffer: 13207 Learn step counter: 13176\n",
      "Episode: 3604\n",
      "Total reward: 630.0 Epsilon: 0.7947471631024888 Size of replay buffer: 13292 Learn step counter: 13261\n",
      "Episode: 3605\n",
      "Total reward: 759.0 Epsilon: 0.7946639176867916 Size of replay buffer: 13711 Learn step counter: 13680\n",
      "Episode: 3606\n",
      "Total reward: 691.0 Epsilon: 0.7945890241325921 Size of replay buffer: 14088 Learn step counter: 14057\n",
      "Episode: 3607\n",
      "Total reward: 249.0 Epsilon: 0.7945834620281946 Size of replay buffer: 14116 Learn step counter: 14085\n",
      "Episode: 3608\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "\n",
    "for i in range(3564, NUM_OF_EPISODES):    \n",
    "    print(\"Episode:\", i)\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        a = agent.choose_action(state)\n",
    "        new_state, reward, done, truncated, info  = env.step(a)\n",
    "        total_reward += reward\n",
    "\n",
    "        if SHOULD_TRAIN:\n",
    "            agent.store_in_memory(state, a, reward, new_state, done)\n",
    "            agent.learn()\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    print(\"Total reward:\", total_reward, \"Epsilon:\", agent.epsilon, \"Size of replay buffer:\", len(agent.replay_buffer), \"Learn step counter:\", agent.learn_step_counter)\n",
    "\n",
    "    if SHOULD_TRAIN and (i + 1) % CKPT_SAVE_INTERVAL == 0:\n",
    "        agent.save_model(os.path.join(model_path, \"model_\" + str(i + 1) + \"_iter.pt\"))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89454723-2186-4483-88de-ba5aa8a53002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
